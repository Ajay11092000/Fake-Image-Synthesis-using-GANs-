# -*- coding: utf-8 -*-
"""Anime SSP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylrPdgD4O5W-Nijf3S0IKYbEMAwB-8y1
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("soumikrakshit/anime-faces")

print("Path to dataset files:", path)

import os

path = os.path.join(path, "data/data")
print(path)

for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

all_images = []

# Loop through each file in the specified directory containing anime face images
for image in os.listdir(path):
    # Construct the full path of the image file
    image_path = os.path.join(path, image)

    # Check if the path is a file (to avoid directories)
    if os.path.isfile(image_path):
        all_images.append(image_path)

print(len(all_images))

import os
import random
import warnings

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
import cv2
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.layers import (Input, Reshape, Dropout, Dense, Flatten,
                                       BatchNormalization, Activation,
                                       LeakyReLU, ReLU, PReLU,
                                       Conv2D, Conv2DTranspose)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import Callback, ModelCheckpoint
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from IPython.display import Image as IPImage
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras import backend as K

# Suppress warnings if necessary
warnings.filterwarnings('ignore')
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import torchvision.transforms as T
from torchvision.utils import make_grid, save_image
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# Initialize an empty list to store paths of all images
all_images = []

# Loop through each file in the specified directory containing anime face images
for image in os.listdir(path):
    # Construct the full path of the image file
    image_path = os.path.join(path, image)

    # Check if the path is a file (to avoid directories)
    if os.path.isfile(image_path):
        all_images.append(image_path)

def gallery(array, ncols=8):
    """
    Reshapes an array of images into a grid for visualization.
    """
    nindex, height, width, intensity = array.shape  # Get the shape of input array
    nrows = nindex // ncols                         # Calculate number of rows
    assert nindex == nrows * ncols                  # Ensure the array size is compatible for a grid
    result = (
        array.reshape(nrows, ncols, height, width, intensity)  # Reshape to grid shape
        .swapaxes(1, 2)  # Swap axes to rearrange images
        .reshape(height * nrows, width * ncols, intensity)  # Final reshaped array
    )
    return result
def  make_array(all_images):
    """
    Selects 64 random images from all_images and converts them to RGB arrays.
    """
    arr = []
    num_images = len(all_images)

    # Ensure we are not trying to select more images than available
    image_count = min(num_images, 64)  # Choose up to 64 images
    if num_images < 64:
        print(f"Warning: Only {num_images} images available, selecting fewer images.")
         # Randomly select images for visualization
    for _ in range(image_count):
        random_image = random.choice(all_images)  # Randomly pick an image from the list
        arr.append(np.asarray(Image.open(random_image).convert('RGB')))  # Convert image to RGB and append

    return np.array(arr)  # Return the array of selected images

# Create an array of randomly selected images for the gallery
array = make_array(all_images)

# Check if the array was created successfully before generating the gallery
if array.size == 0:
    raise ValueError("No valid images to display.")
    # Generate the gallery of images in grid format
result = gallery(array)

# Visualize the result using matplotlib
plt.figure(figsize=(8, 8))  # Set figure size
plt.imshow(result)          # Display the image grid
plt.axis('off')             # Turn off axis
plt.show()                  # Show the plot

# Convert each image path in all_images to an array and store them in train_images
train_images = [img_to_array(load_img(path, target_size=(64, 64))) for path in all_images]

# Convert the list of image arrays to a NumPy array with float32 data type
train_images = np.array(train_images, dtype='float32')

# Normalize the pixel values to be between -1 and 1
train_images = (train_images - 127.5) / 127.5

# Reshape the train_images array to have the shape (num_samples, 64, 64, 3)
train_images = train_images.reshape(train_images.shape[0], 64, 64, 3)

# Configure TensorFlow to use only the GPU (run this only when you want to train on GPU)
gpus = tf.config.experimental.list_physical_devices('GPU')  # List all available GPU devices
if gpus:  # Check if any GPUs are available
    try:
        # Set the first available GPU as the only visible device to TensorFlow
        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
        print("Using GPU")  # Confirm that the GPU is being used
    except RuntimeError as e:
        # Handle any errors that occur during the configuration
        print(e)

# Use the directory containing all the images, not a single image path
data_dir = path

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,  # Directory containing all images
    label_mode=None,  # No labels, since it's for GAN training
    color_mode='rgb',
    batch_size=128,
    image_size=(64, 64),  # Resize all images to 64x64
    shuffle=True  # Shuffle the dataset
)

# Normalize images to range [-1, 1] for GAN training
train_ds = train_ds.map(lambda x: (x / 127.5) - 1)

""" Define Generator and Discriminator"""

# Weights initializer
init = tf.keras.initializers.RandomNormal(stddev=0.02)

def build_generator(seed_size):
    """
    Builds the generator model

    Parameters:
        seed_size: size of the random vector fed into the generator

    Returns:
        model: keras model representing the generator
    """
    init = RandomNormal(mean=0.0, stddev=0.02)  # Define kernel initializer

    model = Sequential()  # Initialize a sequential model
     # Block - 1: First dense layer that expands the seed into a high-dimensional space
    model.add(Dense(4*4*1024, kernel_initializer=init, input_dim=seed_size))  # Dense layer to create initial high-dimensional features
    model.add(BatchNormalization())  # Normalize the activations for faster training
    model.add(ReLU())  # Apply ReLU activation function
    model.add(Reshape((4, 4, 1024)))  # Reshape to (4, 4, 1024) to prepare for convolutional layers

    # Block - 2: Transposed convolution to upsample the feature map
    model.add(Conv2DTranspose(512, kernel_size=5, strides=2, padding='same', use_bias=False, kernel_initializer=init))  # Upsample to (8, 8, 512)
    model.add(BatchNormalization())  # Normalize the activations
    model.add(ReLU())  # Apply ReLU activation
     # Block - 3: Further upsampling
    model.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same', use_bias=False, kernel_initializer=init))  # Upsample to (16, 16, 256)
    model.add(BatchNormalization())  # Normalize the activations
    model.add(ReLU())  # Apply ReLU activation

    # Block - 4: Continue upsampling process
    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same', use_bias=False, kernel_initializer=init))  # Upsample to (32, 32, 128)
    model.add(BatchNormalization())  # Normalize the activations
    model.add(ReLU())  # Apply ReLU activation

    # Block - 5: Final transposed convolution to generate the output image
    model.add(Conv2DTranspose(3, kernel_size=3, strides=2, padding='same', use_bias=False, kernel_initializer=init))  # Upsample to (64, 64, 3)
    model.add(Activation('tanh'))  # Use tanh activation to scale output values to [-1, 1]

    return model  # Return the built generator model
    # Build the generator and print the summary
seed_size = 100  # Example seed size
generator = build_generator(seed_size)
generator.summary()

def build_discriminator(image_length, image_channels):
    """
    Builds the discriminator model

    Parameters:
        image_length: length of a side of the square image
        image_channels: number of channels in the image

    Returns:
        model: keras model representing the discriminator
    """
    init = RandomNormal(mean=0.0, stddev=0.02)  # Define kernel initializer

    model = Sequential()  # Initialize a sequential model

    # Block - 1: First convolutional layer to extract features from the input image
    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same', use_bias=False,
                     input_shape=(image_length, image_length, image_channels),
                     kernel_initializer=init))
    model.add(LeakyReLU(alpha=0.2))  # Use LeakyReLU activation to allow a small gradient when the unit is not active
    # Resulting shape = (image_length / 2, image_length / 2, 64)

    # Block - 2: Second convolutional layer for further feature extraction
    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same', use_bias=False,
                     kernel_initializer=init))
    model.add(BatchNormalization())  # Normalize the activations to stabilize training
    model.add(LeakyReLU(alpha=0.2))  # Use LeakyReLU activation
    # Resulting shape = (image_length / 4, image_length / 4, 128)

    # Block - 3: Third convolutional layer with a larger kernel size
    model.add(Conv2D(256, kernel_size=5, strides=2, padding='same', use_bias=False,
                     kernel_initializer=init))
    model.add(BatchNormalization())  # Normalize the activations
    model.add(LeakyReLU(alpha=0.2))  # Use LeakyReLU activation
    # Resulting shape = (image_length / 8, image_length / 8, 256)

    # Block - 4: Fourth convolutional layer for deeper feature extraction
    model.add(Conv2D(512, kernel_size=5, strides=2, padding='same', use_bias=False,
                     kernel_initializer=init))
    model.add(BatchNormalization())  # Normalize the activations
    model.add(LeakyReLU(alpha=0.2))  # Use LeakyReLU activation
    # Resulting shape = (image_length / 16, image_length / 16, 512)

    # Block - 5: Final convolutional layer to output a single probability
    model.add(Conv2D(1, kernel_size=4, strides=1, padding='valid', use_bias=False,
                     kernel_initializer=init))  # Output layer
    model.add(Flatten())  # Flatten the output for the next layer
    model.add(Activation('sigmoid'))  # Use sigmoid activation to output a probability (real or fake)
    return model  # Return the built discriminator model

# Build the discriminator and print the summary
image_length = 64  # Example image size (e.g., 64x64)
image_channels = 3  # Example number of channels (e.g., RGB images)
discriminator = build_discriminator(image_length, image_channels)
discriminator.summary()

"""Define DCGAN"""

cross_entropy = tf.keras.losses.BinaryCrossentropy()

class DCGAN(keras.Model):
    """Subclass of the keras.Model class to define custom training step and loss functions"""

    def __init__(self, seed_size, image_length, image_channels, **kwargs):
        """
        Parameters:
            seed_size: size of the random vector for the generator
            image_length: length of a side of the square image
            image_channels: number of channels in the image
        """
        super(DCGAN, self).__init__(**kwargs)

        # Build the generator and discriminator models
        self.generator = build_generator(seed_size)
        self.discriminator = build_discriminator(image_length, image_channels)
        self.seed_size = seed_size
            # Train the generator to produce realistic images
    def generator_loss(self, fake_output):
        """
        Parameters:
            fake_output: Tensor containing the discriminator's predictions for the batch of fake images.

        Returns:
            cross entropy loss between labels for real images (1's) and the discriminator's estimate.
        """
        # Penalize the generator for producing images classified as 'fake' by the discriminator
        return cross_entropy(tf.ones_like(fake_output), fake_output)

    def discriminator_loss(self, real_output, fake_output, smooth=0.1):
        """
        Parameters:
            real_output: Tensor containing predictions for real images from the dataset.
            fake_output: Tensor containing predictions for fake images generated by the generator.

        Returns:
            total_loss: Loss of the discriminator for misclassifying images.
        """
        # Smooth the label for real images
        real_loss = cross_entropy(tf.ones_like(real_output) * (1 - smooth), real_output)
        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
        total_loss = real_loss + fake_loss
        return total_loss

    def compile(self, generator_optimizer, discriminator_optimizer):
        """
         Configures model for training by adding optimizers.

        Parameters:
            generator_optimizer: Keras optimizer for the generator.
            discriminator_optimizer: Keras optimizer for the discriminator.
        """
        super(DCGAN, self).compile()
        self.generator_optimizer = generator_optimizer
        self.discriminator_optimizer = discriminator_optimizer

    @tf.function
    def train_step(self, data):
        """
        Performs a training step with a batch of data.

        Parameters:
            data: A batch from the training dataset.
         Returns:
            gen_loss: Loss associated with the generator.
            disc_loss: Loss associated with the discriminator.
        """
        batch_size = tf.shape(data)[0]

        # Generate random input for the generator
        seed = tf.random.normal(shape=(batch_size, self.seed_size))

        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
            # Generate an image using the generator
            generated_image = self.generator(seed, training=True)

            # Get discriminator's predictions for real and fake images
            real_output = self.discriminator(data, training=True)
            fake_output = self.discriminator(generated_image, training=True)
             # Compute losses
            gen_loss = self.generator_loss(fake_output)
            disc_loss = self.discriminator_loss(real_output, fake_output)

            # Compute gradients and update the models
            generator_grad = gen_tape.gradient(gen_loss, self.generator.trainable_variables)
            discriminator_grad = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)

            # Optimize the generator first
            self.generator_optimizer.apply_gradients(zip(generator_grad, self.generator.trainable_variables))
            # Then optimize the discriminator
            self.discriminator_optimizer.apply_gradients(zip(discriminator_grad, self.discriminator.trainable_variables))
            return {
            "generator_loss": gen_loss,
            "discriminator_loss": disc_loss
        }

# parameters and hyperparameters
image_length = 64
image_channels = 3
batch_size = 128
seed_size = 100

NUM_ROWS = 4
NUM_COLS = 7
MARGIN = 16

fixed_seed = tf.random.normal(shape=(NUM_ROWS * NUM_COLS, seed_size))
# We are defining a fixed seed because we want to see the generator's images quality by generating the same images every epoch

"""Initialize the DCGAN

Initializing the DCGAN involves creating an instance of the DCGAN class, which sets up the generator and discriminator models. This step prepares the model for training by defining its architecture and compiling it with specified optimizers.
"""

# Initialize the Adam optimizer for the generator with a learning rate of 0.0002 and beta_1 set to 0.5
generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)

# Initialize the Adam optimizer for the discriminator with the same parameters as the generator
discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)

# Create an instance of the DCGAN class, specifying the seed size, image dimensions, and channels
dcgan = DCGAN(seed_size, image_length, image_channels)

# Compile the DCGAN model by assigning the optimizers to the generator and discriminator, preparing it for training
dcgan.compile(generator_optimizer, discriminator_optimizer)

"""Key Points:

Adam Optimizer: Used for both the generator and discriminator, ensuring effective adaptive learning.

DCGAN Instance: Creates a new instance of the model, establishing its structure with the specified parameters.

Model Compilation: Prepares the DCGAN for training by assigning the optimizers, allowing for weight updates during training.
"""

class SaveImages(tf.keras.callbacks.Callback):
    def __init__(self, generator, noise, num_rows, num_cols, margin):
        super(SaveImages, self).__init__()  # Properly call the parent constructor
        self.generator = generator
        self.noise = noise
        self.num_rows = num_rows
        self.num_cols = num_cols
        self.margin = margin

    def on_epoch_end(self, epoch, logs=None):
        # Generate images at the end of each epoch
        generated_images = self.generator(self.noise, training=False)
        generated_images = (generated_images + 1) / 2.0  # Rescale to [0, 1]
        # Plot images in a grid
        fig, ax = plt.subplots(self.num_rows, self.num_cols, figsize=(self.num_cols * 2, self.num_rows * 2))
        for i in range(self.num_rows):
            for j in range(self.num_cols):
                ax[i, j].imshow(generated_images[i * self.num_cols + j])
                ax[i, j].axis('off')
        plt.subplots_adjust(wspace=self.margin, hspace=self.margin)
        plt.savefig(f"generated_image_epoch_{epoch+1}.png")
        plt.close()

# Checkpoint callback to save model weights
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath="model_checkpoint.weights.h5",  # Use the correct file extension
    save_weights_only=True,  # Save only the weights
    save_best_only=True,  # Save only the best model according to the monitored metric
    verbose=1
)

# Fixed seed for consistent image generation across epochs
fixed_seed = tf.random.normal(shape=(NUM_ROWS * NUM_COLS, seed_size))

# Use SaveImages callback
save_images_callback = SaveImages(generator, fixed_seed, NUM_ROWS, NUM_COLS, MARGIN)

"""## Epochs and Results for DCGAN"""

with tf.device('/GPU:0'):
    history = dcgan.fit(
        train_ds,
        epochs=50,
        batch_size=batch_size,
        callbacks=[
            save_images_callback,  # Use the instantiated callback
            checkpoint_callback  # Use the instantiated checkpoint callback
        ]
    )

dcgan.generator.save("generator_model.h5")
dcgan.discriminator.save("discriminator_model.h5")
dcgan.generator.save_weights("generator.weights.h5")
dcgan.discriminator.save_weights("discriminator.weights.h5")
dcgan.save_weights("dcgan_full.weights.h5")

# generator.save("dcgan_generator.h5")
# discriminator.save("dcgan_discriminator.h5")
# dcgan.save("dcgan_model.h5");
# generator.save_weights("dcgan_generator.weights.h5")  # Correct filename
# discriminator.save_weights("dcgan_discriminator.weights.h5")  # Correct filename

def plot_losses(history):
    # Assuming that the history object has keys for the generator and discriminator losses
    generator_losses = history.history['generator_loss']  # Adjust according to your history keys
    discriminator_losses = history.history['discriminator_loss']  # Adjust according to your history keys

    plt.figure(figsize=(12, 6))
    plt.plot(generator_losses, label='Generator Loss', color='blue')
    plt.plot(discriminator_losses, label='Discriminator Loss', color='red')
    plt.title('Generator and Discriminator Losses')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

def generate_faces():
    """Generates random Anime faces"""
      # Generate 64 images by giving 64 inputs
    noise = tf.random.normal([64, seed_size])
    generated_images = dcgan.generator(noise)

    fig = plt.figure(figsize=(12, 12))
    for i in range(generated_images.shape[0]):
        plt.subplot(8, 8, i + 1)
        # Convert to range [0,1] for plt.imshow()
        plt.imshow((generated_images[i, :, :, :] * 0.5 + 0.5))  # Rescale from [-1, 1] to [0, 1]
        plt.axis("off")
    plt.show()

# After training, call the plot function
plot_losses(history)
# Step 3: Call the function to generate faces
generate_faces()

"""# Testing"""

# Step 1: List files in the Kaggle input directory to verify weight locations
# input_dir = path
# # for folder in os.listdir(input_dir):
# #     # print(f"Folder: {folder}")
# #     folder_path = os.path.join(input_dir, folder)
# #     if os.path.isdir(folder_path):
# #         for file in os.listdir(folder_path):
# #             # print(f" - {file}")

# # Step 2: Load model weights - ensure you have the correct paths
# weights_dir = "/content/drive/MyDrive/SSP_PROJECT"  # Update with the correct folder name
# try:
#     dcgan.generator.load_weights("/content/dcgan_generator.weights.h5")
#     dcgan.discriminator.load_weights("/content/dcgan_discriminator.weights.h5")
# except FileNotFoundError as e:
#     print(e)  # This will print if the weights are not found

def generate_faces():
    """Generates random Anime faces"""
      # Generate 64 images by giving 64 inputs
    noise = tf.random.normal([64, seed_size])
    generated_images = dcgan.generator(noise)

    fig = plt.figure(figsize=(12, 12))
    for i in range(generated_images.shape[0]):
        plt.subplot(8, 8, i + 1)
        # Convert to range [0,1] for plt.imshow()
        plt.imshow((generated_images[i, :, :, :] * 0.5 + 0.5))  # Rescale from [-1, 1] to [0, 1]
        plt.axis("off")
    plt.show()

# Step 3: Call the function to generate faces
generate_faces()

dcgan.generator.save("generator_model.keras")
dcgan.discriminator.save("discriminator_model.keras")
dcgan.generator.save_weights("generator.weights.h5")
dcgan.discriminator.save_weights("discriminator.weights.h5")
# dcgan.save_weights("dcgan_full.weights.h5")

"""GAN

What is GAN?

Generative Adversarial Networks (GAN) are popular and powerful machine learning techniques used in image, text, video, and voice generation. Since the first GAN was introduced in 2014, this research area has evolved rapidly, and it remains the most flexible neural network architecture used today. GANs are categorized under unsupervised learning. This is one of the main reasons that many researchers have conducted their work using generative models.

How do GANs work?

GANs utilize two independent neural networks to generate new data, named the Generator and the Discriminator.

What does the Generator Neural Network do?

The Generator neural network generates synthetic data (fake samples) using random noise.

What does the Discriminator Neural Network do?

The Discriminator neural network acts as a binary classifier and classifies the input sample as real or fake.

The generator produces images, referred to as fake images, from random noise.

These images, along with real images, are provided as input to the discriminator, which outputs a scalar value between 0 and 1. This value represents the probability that the image is real (real: 1, fake: 0).

The generator and discriminator are improved iteratively using the loss derived from the discriminator’s output. This process continues until the images generated by the generator yield an output value of 1 from the discriminator.

When the generator consistently achieves an output of 1, it indicates that the discriminator can no longer distinguish between real and fake images, as it perceives the fake images to be real.

At this stage, the generator is able to produce images that closely match the distribution of real data, simulating real images effectively.
# Define the data directory
"""

# Assuming your images are in a folder called 'data' under path
path = "/root/.cache/kagglehub/datasets/soumikrakshit/anime-faces/versions/1/data/"
DATA_DIR = path  # Update 'images' with the correct folder name if different

# Check if the data directory exists
if not os.path.exists(DATA_DIR):
    raise FileNotFoundError(f"Data directory not found at {DATA_DIR}")

# Parameters
image_size = 64
batch_size = 128
stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)

# Create dataset with transformations
# If images are directly under DATA_DIR, use DATA_DIR
train_ds = ImageFolder(DATA_DIR, transform=T.Compose([
    T.Resize(image_size),
    T.CenterCrop(image_size),
    T.ToTensor(),
    T.Normalize(*stats)
    ]))

# DataLoader
train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)

def denorm(img_tensors):
    return img_tensors * stats[1][0] + stats[0][0]

def show_images(images, nmax=64):
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.set_xticks([]); ax.set_yticks([])
    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))

def show_batch(dl, nmax=64):
    for images, _ in dl:
        show_images(images, nmax)
        break

show_batch(train_dl)

def get_default_device():
    """Pick GPU if available, else CPU"""
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list, tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

class DeviceDataLoader:
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device
    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl:
            yield to_device(b, self.device)

    def __len__(self):
        """Number of batches"""
        return len(self.dl)

device = get_default_device()
train_dl = DeviceDataLoader(train_dl, device)

discriminator = nn.Sequential(
    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(64),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(128),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(256),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(512),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),
    nn.Flatten(),
    nn.Sigmoid()
)
discriminator = nn.Sequential(
    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(64),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(128),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(256),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(512),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),
    nn.Flatten(),
    nn.Sigmoid()
)

discriminator = to_device(discriminator, device)

latent_size = 128
generator = nn.Sequential(
    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),
    nn.BatchNorm2d(512),
    nn.ReLU(True),
    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(256),
    nn.ReLU(True),
    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(128),
    nn.ReLU(True),
    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),
    nn.BatchNorm2d(64),
    nn.ReLU(True),
    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),
    nn.Tanh()
)
xb = torch.randn(batch_size, latent_size, 1, 1)  # random latent tensors
fake_images = generator(xb)
print(fake_images.shape)
show_images(fake_images)

generator = to_device(generator, device)

def train_discriminator(real_images, opt_d):
    # Clear discriminator gradients
    opt_d.zero_grad()

    # Pass real images through discriminator
    real_preds = discriminator(real_images)
    real_targets = torch.ones(real_images.size(0), 1, device=device)
    real_loss = F.binary_cross_entropy(real_preds, real_targets)
    real_score = torch.mean(real_preds).item()
        # Generate fake images
    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)
    fake_images = generator(latent)

    # Pass fake images through discriminator
    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)
    fake_preds = discriminator(fake_images)
    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)
    fake_score = torch.mean(fake_preds).item()

    # Update discriminator weights
    loss = real_loss + fake_loss
    loss.backward()
    opt_d.step()
    return loss.item(), real_score, fake_score

def train_generator(opt_g):
    # Clear generator gradients
    opt_g.zero_grad()

    # Generate fake images
    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)
    fake_images = generator(latent)

    # Try to fool the discriminator
    preds = discriminator(fake_images)
    targets = torch.ones(batch_size, 1, device=device)
    loss = F.binary_cross_entropy(preds, targets)

    # Update generator weights
    loss.backward()
    opt_g.step()
    return loss.item()

sample_dir = 'generated'
os.makedirs(sample_dir, exist_ok=True)

def save_samples(index, latent_tensors, show=True):
    fake_images = generator(latent_tensors)
    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)
    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)
    print('Saving', fake_fname)
    if show:
        fig, ax = plt.subplots(figsize=(8, 8))
        ax.set_xticks([]); ax.set_yticks([])
        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))
        fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)

# save_samples(0, fixed_latent)

def fit(epochs, lr, start_idx=1):
    torch.cuda.empty_cache()

    # Losses & scores
    losses_g = []
    losses_d = []
    real_scores = []
    fake_scores = []

    fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)

    # Create optimizers
    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))
    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))
    for epoch in range(epochs):
        for real_images, _ in tqdm(train_dl):
            # Train discriminator
            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)
            # Train generator
            loss_g = train_generator(opt_g)

        # Record losses & scores
        losses_g.append(loss_g)
        losses_d.append(loss_d)
        real_scores.append(real_score)
        fake_scores.append(fake_score)

        # Log losses & scores (last batch)
        print("Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}".format(
            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))
         # Save generated images
        save_samples(epoch+start_idx, fixed_latent, show=False)

    return losses_g, losses_d, real_scores, fake_scores

lr = 0.0002
epochs = 40
history = fit(epochs, lr)

# Save the model checkpoints
torch.save(generator.state_dict(), 'G.ckpt')
torch.save(discriminator.state_dict(), 'D.ckpt')

import matplotlib.image as mpimg
import matplotlib.pyplot as plt

img = mpimg.imread('./generated/generated-images-0005.png')
imgplot = plt.imshow(img)
plt.axis('off')  # Optionally hide the axes for better visualization
plt.show()

img = mpimg.imread('./generated/generated-images-0015.png')
imgplot = plt.imshow(img)
plt.axis('off')  # Optionally hide the axes for better visualization
plt.show()

img = mpimg.imread('./generated/generated-images-0035.png')
imgplot = plt.imshow(img)
plt.axis('off')  # Optionally hide the axes for better visualization
plt.show()

img = mpimg.imread('./generated/generated-images-0040.png')
imgplot = plt.imshow(img)
plt.axis('off')  # Optionally hide the axes for better visualization
plt.show()

# Unpack the history returned by the fit function
losses_g, losses_d, real_scores, fake_scores = history

# Plot the losses
plt.figure(figsize=(12, 6))

# Plot Generator and Discriminator Losses
plt.subplot(1, 2, 1)
plt.plot(losses_d, label='Discriminator Loss', color='red')
plt.plot(losses_g, label='Generator Loss', color='blue')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Generator and Discriminator Losses')
plt.legend()
# Plot Real and Fake Scores
plt.subplot(1, 2, 2)
plt.plot(real_scores, label='Real Scores', color='green')
plt.plot(fake_scores, label='Fake Scores', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.title('Real and Fake Scores')
plt.legend()

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()  # Display the plots